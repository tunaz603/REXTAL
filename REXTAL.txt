Rextal:

1) extract 1-copy, bait, sd from UC genome browser
2) Use online RepeatMasker (RM)
3) Use tandem repeat finder
4) python code for wrapping the line (text_wrap_affter_tandem_repeat_finder.py)
5) run blat using job script i.e. sbatch slurm-jobBLAT.sh

............frequency................
6) extract long header from .psl file (after BLAT):
grep -oP "E00489.\S*" 18p_blat.psl > header_18p.fastq  
7)extract only name with 10x barcode :
grep -oP 'E00489.*-1' header_18p.fastq > name10X_barcode_18p.txt 
8)Replace a "_1_" or "_2_" with "_":

sed 's/\_1_/_/g' name10X_barcode_18p.txt > modified10X_barcode_18p.txt
sed 's/\_2_/_/g' modified10X_barcode_18p.txt > new10X_barcode_18p.txt
9)sort aligned sequences according to barcode (frequency analysis): 
sort -t_ -k2,2 new10X_barcode_18p.txt > sorted_10X_18p.txt

10)get uniq sequence but have same barcode (frequency analysis):

uniq sorted_10X_18p.txt > unq_seq_10X_18p.txt

11)Extract those barcode only from unique sequence will repeat (frequency analysis):
 
awk -F '_' '{sub(/"".*$/, "", $2); print $2}' unq_seq_10X_18p.txt > unqseq_samebarcodes_18p.txt

12)add prefix "_" in each line:(this will also be used in histogram ipython:)
sed -e 's/^/_/' unqseq_samebarcodes_18p.txt > unqseq_modified_samebarcodes_18p.txt

13)print occurance of same barcodes for uniq sequence (frequency analysis):

uniq -c unqseq_modified_samebarcodes_18p.txt | sort > unqseq_occur_samebarcodes_18p.txt
...............range analysis..............
*********Pulling out ranged wise barcode from BLAT output (big data):***********
cd /scratch-lustre/tisla003/Original_Data/summer2018/BLAT_output/18p

14) python extract_bc_range.py <low_range> <high_range>
 
15) sort python output barcode-wise:
sort range_bc_3_70.txt > sorted_bc_3_70.txt

.................Cluster analysis.....................
Do clustering now on specific range:
16) Cluster analysis of whole blat output:
get those output directly from BLAT in range 3-70:
grep -f "range_bc_3_70.txt" 18p_blat.psl > blat_output_3_70.txt

17) extract column 21 (which is starting position of match in 18p) delimited by tab (cut command give default 'tab' delimiter):
cut -f21 blat_output_3_70.txt > start_position_3_70.txt

18) extract barcode from Blat output range 3-70:
grep -o '\_B.*-1' blat_output_3_70.txt > extract_barcode_blat_output_3_70.txt

19) keep 1st column only
cut -f1 extract_barcode_blat_output_3_70.txt > barcode_blat_output_3_70.txt 

20) Run python code for thresholding:
python cluster_bc_multiple_3_70.py

21) take good barcode file with 100kb threshold i.e. barcode_yes_3_70_less_than_100kb.txt
.................after clustering..................
22) if this is first one: extract some parts from original data
nohup grep -f "barcode_yes_3_70_less_than_100kb.txt" /scratch-lustre/tisla003/Original_Data/longranger/longranger_output/barcoded_output/outs/nospace_modified_barcode_fasta_big.fasta > ranged_common_part_bc_100kb_3_70.fastq &

23) extract common part:
awk -F '_' '{sub(/"".*$/, "", $1); print $1}' ranged_common_part_bc_100kb_3_70.fastq > common_part_100kb_3_70.fastq

24)drop first '>' from each file:
cut -c 2- common_part_100kb_3_70.fastq > new_common_part_100kb_3_70.fastq

25) extract Lane 001 and Lane 002 from common part :

awk -F ':' '{sub(/"".*$/, "", $4); if($4==1) print $0}' new_common_part_100kb_3_70.fastq > com_part_L001_100kb_3_70.fastq

awk -F ':' '{sub(/"".*$/, "", $4); if($4==2) print $0}' new_common_part_100kb_3_70.fastq > com_part_L002_100kb_3_70.fastq

26) Extract unique common part :
sort com_part_L001_100kb_3_70.fastq | uniq > unq_com_part_L001_100kb_3_70.fastq
sort com_part_L002_100kb_3_70.fastq | uniq > unq_com_part_L002_100kb_3_70.fastq

27) mkdir split_L001 split_L002

28) copy to folder split_L001 & split_L002
cp -r unq_com_part_L001_100kb_3_70.fastq split_L001
cp -r unq_com_part_L002_100kb_3_70.fastq split_L002

//for L001
29) cd split_L001
30) split the file unq_com_part_L001_100kb_3_70.fastq in multiple files 
split -l 2690 -a 3 unq_com_part_L001_100kb_3_70.fastq 
31)create the script file_match_3_70_L001.sh  
touch file_match_3_70_L001.sh
32) find the headers those match with the big input illumina data
run :
nohup ./file_match_3_70_L001.sh > illumina_bc_linenum_L001_big.fastq &

33) Use python code get_line_number.py to extract line number only from illumina_bc_linenum_L001_big.fastq. We will get output_line_number_L001_3_70.txt

34)Get mate pairs fastq using Line number. Use job scripts job_I1.sh, job_R1.sh, job_R2.sh
python extract_I1_L001_3_70.py
python extract_R1_L001_3_70.py
python extract_R2_L001_3_70.py

//for L002
35) cd split_L002
36) split the file unq_com_part_L002_100kb_3_70.fastq in multiple files 
split -l 2690 -a 3 unq_com_part_L002_100kb_3_70.fastq 
37)create the script file_match_3_70_L002.sh  
touch file_match_3_70_L002.sh
38) find the headers those match with the big input illumina data
run :
nohup ./file_match_3_70_L002.sh > illumina_bc_linenum_L002_big.fastq &

39) Use python get_line_number.py to extract line number only from illumina_bc_linenum_L002_big.fastq. We will get output_line_number_L002_3_70.txt

40)Get mate pairs fastq using Line number. Write job scripts job_I1.sh, job_R1.sh, job_R2.sh
python extract_I1_L002_3_70.py
python extract_R1_L002_3_70.py
python extract_R2_L002_3_70.py

41) run the job scripts. 
sbatch job_I1.sh
sbatch job_R1.sh
sbatch job_R2.sh

42) copy those selected L001 & L002 files in Supernova folder and rename them
L001: HG00353_S1_L001_I1_001.fastq, HG00353_S1_L001_R1_001.fastq, HG00353_S1_L001_R2_001.fastq
L002: HG00353_S1_L002_I1_001.fastq, HG00353_S1_L002_R1_001.fastq, HG00353_S1_L002_R2_001.fastq
.............Do the Assembly of mate pairs using supernova...............
43) module load supernova

44)For Assembly: create a job script (job_supernova.sh) for supernova run
supernova run --id=sample345 \--fastqs=/scratch-lustre/tisla003/Original_Data/summer2018/Supernova_for_journal/18p_input

run through job script
sbatch /scratch-lustre/tisla003/Original_Data/summer2018/Supernova_for_journal/Job_script/job_supernova.sh

45) To generate FASTA format output

supernova mkoutput --asmdir=/scratch-lustre/tisla003/Original_Data/summer2018/Supernova_for_journal/18p_output/sample345/outs/assembly  \--outprefix=/scratch-lustre/tisla003/Original_Data/summer2018/Supernova_for_journal/18p_output/18p_pseudohap2 \--style=pseudohap2

................end of supernova assembly..............
46) gunzip 18p*
47)Generate fasta sequence lengths:

cat 18p_pseudohap2.1.fasta | awk '$0 ~ ">" {print c; c=0;printf substr($0,2,100) "\t"; } $0 !~ ">" {c+=length($0);} END { print c; }' > size.fasta

48)sort file by column 7 numerically in reverse order:

sort -k7nr size.fasta > sorted_size.fasta
.......................BLAST Alignment..................

.............QUAST analysis.........................

1) Before starting quast analysis to convert Multiline Fasta To Single Line Fasta: (we need this for REXTAL)
awk '/^>/ {printf("\n%s\n",$0);next; } { printf("%s",$0);}  END {printf("\n");}' < 18p_pseudohap2.1.fasta > try_18p_pseudohap2.1.fasta

2) there will be an empty line at the beginning If you redirect the result to a file,this should be removed like: 
tail -n +2 filein.fa > fileout.fa

tail -n +2 try_18p_pseudohap2.1.fasta > 18p_REXTAL.fasta

3) Remove 10N from assembled file: 
multiple pattern inplace replace using sed:
sed -i 's/ab/~~/g; s/bc/ab/g; s/~~/bc/g' inputfile

sed -i 's/ANNNNNNNNNNA/AA/g; s/ANNNNNNNNNNT/AT/g; s/ANNNNNNNNNNC/AC/g; s/ANNNNNNNNNNG/AG/g;s/TNNNNNNNNNNA/TA/g; s/TNNNNNNNNNNT/TT/g; s/TNNNNNNNNNNC/TC/g; s/TNNNNNNNNNNG/TG/g; s/CNNNNNNNNNNA/CA/g; s/CNNNNNNNNNNT/CT/g; s/CNNNNNNNNNNC/CC/g; s/CNNNNNNNNNNG/CG/g; s/GNNNNNNNNNNA/GA/g; s/GNNNNNNNNNNT/GT/g; s/GNNNNNNNNNNC/GC/g; s/GNNNNNNNNNNG/GG/g' 18p_REXTAL.fasta 


.............................Icarus view...................
You can run Quast with --debug option and then find <assembly_name>_broken version of your assembly under <quast_output_dir>/quast_corrected_input/
After that, rerun Quast with the same reference but use <assembly_name>_broken instead of your original assembly (and do not use --scaffolds this time).

python /home/tisla003/quast/quast-4.6.3/quast.py /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_REXTAL.fasta -R /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_total.fasta -o /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_REXTAL_output_try --scaffolds --scaffold-gap-max-size 10000 --extensive-mis-size 1000 --debug

python /home/tisla003/quast/quast-4.6.3/quast.py /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_GW.fasta -R /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_total.fasta -o /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_GW_output_try --scaffolds --scaffold-gap-max-size 10000 --extensive-mis-size 1000 --debug

python /home/tisla003/quast/quast-4.6.3/quast.py /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_total_tandem.fasta -R /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_total.fasta -o /scratch-lustre/tisla003/Original_Data/summer2018/Quast_Icarus/18p/18p_total_try --scaffolds --scaffold-gap-max-size 10000 --extensive-mis-size 1000 --debug

Then take 18p_total_tandem_broken, 18p_REXTAL_broken, 18p_GW_broken files and run in online QUAST with reference and without scaffold option.  
